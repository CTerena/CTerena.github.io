<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>神经网络入门 | 停云</title>
<link rel="shortcut icon" href="https://CTerena.github.io/favicon.ico?v=1633922749226">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://CTerena.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="神经网络入门 | 停云 - Atom Feed" href="https://CTerena.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="














Part 1. 阶跃函数的实现
def step_function(x):
    y = x &gt; 0
    return y.astype(np.int)

原理示例：
import numpy as..." />
    <meta name="keywords" content="机器学习,Python" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://CTerena.github.io">
  <img class="avatar" src="https://CTerena.github.io/images/avatar.png?v=1633922749226" alt="">
  </a>
  <h1 class="site-title">
    停云
  </h1>
  <p class="site-description">
    尽道清歌传皓齿，风起，雪飞炎海变清凉
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              神经网络入门
            </h2>
            <div class="post-info">
              <span>
                2021-10-09
              </span>
              <span>
                10 min read
              </span>
              
                <a href="https://CTerena.github.io/tag/3JmSj_8IP/" class="post-tag">
                  # 机器学习
                </a>
              
                <a href="https://CTerena.github.io/tag/200FfDP2C/" class="post-tag">
                  # Python
                </a>
              
            </div>
            
              <img class="post-feature-image" src="http://imgsrc.baidu.com/forum/pic/item/d88d7b899e510fb30c2bdd3ede33c895d1430c2c.jpg" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<!-- more -->
<h1 id="part-1-阶跃函数的实现">Part 1. 阶跃函数的实现</h1>
<pre><code class="language-cpp">def step_function(x):
    y = x &gt; 0
    return y.astype(np.int)
</code></pre>
<p>原理示例：</p>
<pre><code class="language-cpp">import numpy as np

def step_function(x):
    y = x &gt; 0
    return y.astype(np.int)

x = np.array([-1.0, 1.0, 2.0])
y = x &gt; 0
print(y) #输出结果：[False  True  True]
</code></pre>
<p>对Numpy数组进行不等号运算时，数组的各个元素都会进行不等号运算，生成一个布尔型数组，进而用<code>astype()</code>方法将布尔型转换为int型即可得到我们想要的阶跃函数（输出int型的0或1的函数）</p>
<p>获取阶跃函数的图形：</p>
<pre><code class="language-cpp">import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x &gt; 0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://CTerena.github.io/post-images/1633770470145.png" alt="" loading="lazy"></figure>
<!-- more -->
<h1 id="part-2-sigmoid函数的实现">Part 2. sigmoid函数的实现</h1>
<pre><code class="language-cpp">def sigmoid(x):
    return 1 / (1 + np.exp(-x))
</code></pre>
<p>借助Numpy的广播功能，sigmoid函数可支持Numpy数组的运算</p>
<!-- more -->
<p>获取sigmoid函数图形：</p>
<pre><code class="language-cpp">import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x,y)
plt.show()
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://CTerena.github.io/post-images/1633770889449.png" alt="" loading="lazy"></figure>
<h2 id="sigmoid函数和阶跃函数的比较">sigmoid函数和阶跃函数的比较</h2>
<ol>
<li>与阶跃函数不同，sigmoid函数图像是一条平滑的曲线，该平滑性对神经网络的学习具有重要意义。</li>
<li>相对于阶跃函数只能返回0或1，sigmoid函数可以返回连续的实数值信号。</li>
<li>两函数的共同点在于，当输入信号为重要信息（接近1）时，阶跃函数和sigmoid函数都会输出较大的值；当输出信号为不重要的信息时，两者都输出较小的值。另外一个共同点是，输出信号的值始终在0到1之间</li>
</ol>
<!-- more -->
<h1 id="part-3-relu函数的实现">Part 3. ReLU函数的实现</h1>
<pre><code class="language-cpp">def relu(x):
    return np.maximum(0,x)
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://CTerena.github.io/post-images/1633771491628.png" alt="" loading="lazy"></figure>
<h1 id="part-4-神经网络的内积">Part 4. 神经网络的内积</h1>
<p>示例：用NumPy矩阵实现神经网络（省略偏置与激活函数，只有权重）<br>
<img src="https://CTerena.github.io/post-images/1633775670115.png" alt="" loading="lazy"></p>
<pre><code class="language-cpp">import numpy as np

X = np.array([1,2])
W = np.array([[1, 3, 5], [2, 4, 6]])

Y = np.dot(X, W)
print(Y)
</code></pre>
<h1 id="part-5-3层神经网络的实现">Part 5. 3层神经网络的实现</h1>
<figure data-type="image" tabindex="4"><img src="https://CTerena.github.io/post-images/1633775953786.png" alt="" loading="lazy"></figure>
<h2 id="1-从定义符号开始">1. 从定义符号开始：</h2>
<p><img src="https://CTerena.github.io/post-images/1633776090499.png" alt="" loading="lazy"><br>
如图，权重和隐藏层的神经元右上角有一个&quot;(1)&quot;，它表示权重和神经元的层号（即第一层的权重、第一次的神经元）。此外，权重右下角有两个数字，它们是后一层的神经元和前一层的神经元的索引号，按照“后一层的索引号、前一层的索引号”的顺序排列。</p>
<!-- more -->
<h2 id="2-各层间信号传递的实现">2. 各层间信号传递的实现</h2>
<p><img src="https://CTerena.github.io/post-images/1633776270775.png" alt="" loading="lazy"><br>
现在进行从输入层到第一层的第一个神经元的信号传递过程（增加了表示偏置的神经元&quot;1&quot;)可以注意到偏置右下角的索引号只有一个，这是由于前一层只有一个偏置神经元<br>
由前所述，现在通过加权信号和偏置的和按如下方式进行计算：</p>
<!-- more -->
<p><strong>a<sub>1</sub><sup>(1)</sup>=w<sub>11</sub><sup>(1)</sup>x<sub>1</sub>+w<sub>12</sub><sup>(1)</sup>x<sub>2</sub>+b<sub>1</sub><sup>(1)</sup></strong><br>
此外，如果使用矩阵的乘法运算，则可以将第一层的加权和表示成下式：<br>
<strong>A<sup>(1)</sup>=XW<sup>(1)</sup>+B<sup>(1)</sup></strong><br>
其中<br>
<img src="https://CTerena.github.io/post-images/1633776861616.png" alt="" loading="lazy"><br>
下面用NumPy多维数组来实现上式，这里将输入信号、权重和偏置设置为任意值</p>
<pre><code class="language-cpp">x = np.array([1.0, 0.5])
w1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.5]])
b1 = np.array([0.1, 0.2, 0.3])

a1 = np.dot(x, w1) + b1 #a1 = [0.3 0.7 1.1]
</code></pre>
<p>接下来，观察第一层中激活函数的计算过程。该过程可由下图表示<br>
<img src="https://CTerena.github.io/post-images/1633777156190.png" alt="" loading="lazy"><br>
如图所示，隐藏层的加权和用a表示，被激活函数转换后的信号用z表示。此外，图中h()表示激活函数，这里我们使用sigmoid函数。<br>
<code>z1 = sigmoid(a1) #z1 = [0.57444252 0.66818777 0.75026011]</code><br>
下面，进行第一层到第二层的信号传递（如图所示）<br>
<img src="https://CTerena.github.io/post-images/1633777516340.png" alt="" loading="lazy"></p>
<pre><code class="language-cpp">w2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
b2 = np.array([0.1, 0.2])
a2 = np.dot(z1, w1) + b2
z2 = sigmoid(a2)
</code></pre>
<p>除了第一层的输出（z1）变为第二层的输入（a2）这一点外，这部分代码与前一部分完全相同。</p>
<!-- more -->
<p>最后是第二层到输出层的信号传递。输出层的实现也和之前的实现基本相同。不过，最后的激活函数和之前的隐藏层略有不同。<br>
<img src="https://CTerena.github.io/post-images/1633777886238.png" alt="" loading="lazy"></p>
<pre><code class="language-cpp">def identity_function(x):
    return x

w3 = np.array([[0.1, 0.3], [0.2, 0.4]])
b3 = np.array([0.1, 0.2])

a3 = np.dot(z2, w3) + b3
y = identity_function(a3)
</code></pre>
<p>这里我们定义了identity_function()函数（恒等函数），并将其作为输出层的激活函数。（这里这样实现只是为了和之前的流程保持格式统一。</p>
<!-- more -->
<h2 id="代码整合小结">代码整合小结</h2>
<p>至此，3层神经网络的实现已经介绍完毕。下面，按照神经网络的实现惯例，我们将权重用大写字母表示，其余量用小写字母表示来整理代码：（类优化）</p>
<pre><code class="language-cpp">def identity_function(x):
    return x

def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0,3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)
</code></pre>
<h1 id="part-6-输出层的设计">Part 6. 输出层的设计</h1>
<p>神经网络可以用在分类和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。</p>
<ul>
<li>回归问题可理解为“根据某个输入预测一个数值”的问题。</li>
</ul>
<!-- more -->
<h2 id="softmax函数">softmax函数</h2>
<p>分类问题中使用的softmax函数可用下式表示：<br>
<img src="https://CTerena.github.io/post-images/1633869664594.png" alt="" loading="lazy"><br>
该式表示假设输出层共有n个神经元，计算第k个神经元的输出y<sub>k</sub>。<br>
用图表示softmax函数：<br>
<img src="https://CTerena.github.io/post-images/1633869768645.png" alt="" loading="lazy"><br>
softmax函数实现：</p>
<pre><code class="language-cpp">import numpy as np
def softmax(a):
    exp_a = np.exp(a) #分子
    sum_exp_a = np.sum(exp_a) #分母
    y = exp_a / sum_exp_a #计算
    return y

a = [12.32, 3.67, 9.83]
print(softmax(a))
# [9.23288490e-01 1.61692603e-04 7.65498178e-02]
</code></pre>
<h2 id="注意事项">注意事项</h2>
<p>softmax函数的实现要进行指数函数的运算，指数函数的值很容易变得非常大。如果在这些超大数值之间进行除法运算，结果会出现“不确定”情况<br>
为避免这种问题，我们可以像对算式进行如下变形优化：<br>
<img src="https://CTerena.github.io/post-images/1633870346256.png" alt="" loading="lazy"></p>
<!-- more -->
<p>示例（未优化）：</p>
<pre><code class="language-cpp">import numpy as np
def softmax(a):
    exp_a = np.exp(a) #分子
    sum_exp_a = np.sum(exp_a) #分母
    y = exp_a / sum_exp_a #计算
    return y

a = np.array([1010, 1000, 990])

print(softmax(a))
'''
输出结果：
[nan nan nan]
C:\Users\Charles.Wen\untitled4.py:3: RuntimeWarning: overflow encountered in exp
  exp_a = np.exp(a) #分子
C:\Users\Charles.Wen\untitled4.py:5: RuntimeWarning: invalid value encountered in true_divide
  y = exp_a / sum_exp_a #计算
'''
已优化：
```cpp
import numpy as np
def softmax(a):
    exp_a = np.exp(a) #分子
    sum_exp_a = np.sum(exp_a) #分母
    y = exp_a / sum_exp_a #计算
    return y

a = np.array([1010, 1000, 990])
c = np.max(a)
a -= c
print(softmax(a))
#正常输出：[9.99954600e-01 4.53978686e-05 2.06106005e-09]
</code></pre>
<!-- more -->
<p>自动进行优化的softmax函数：</p>
<pre><code class="language-cpp">def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
</code></pre>
<!-- more -->
<h2 id="softmax函数的特征">softmax函数的特征</h2>
<ol>
<li>softmax函数的输出是0.0到1.0之间的实数</li>
<li>softmax函数的输出值总和是1，正是有了这个性质，我们才可以把softmax函数的输出解释成“概率”</li>
</ol>
<h1 id="part-7-数字识别程序">Part 7. 数字识别程序</h1>
<p>该数字识别借助MNIST数字集实现，需要自行下载该资源到相应的地址，且权重、偏置包调用调配完毕的数据，从文件中导入即可。本程序仅实现测试数据即参数的导入与神经网络的向前计算过程</p>
<pre><code class="language-cpp">import numpy as np
import sys, os
sys.path.append(os.pardir)
from dataset.mnist import load_mnist
from PIL import Image

def sigmoid(x):
    return 1 / (1 + exp(-x))

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

def get_data():
    (x_train, t_train), (x_test, t_test) = \
        load_mnist(normalize = True, flatten = True, one_hot_label = False)
    return x_test, t_test

def init_network():
    with open(&quot;sample_weight.pkl&quot;, 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y =softmax(a3)
    
    return y

x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)
    if p == t[i]:
        accuracy_cnt += 1
print(&quot;Accuracy:&quot; + str(float(accuracy_cnt) / len(x))) #评估测试并输出神经网络的测试精度
</code></pre>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#part-1-%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9E%E7%8E%B0">Part 1. 阶跃函数的实现</a></li>
<li><a href="#part-2-sigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9E%E7%8E%B0">Part 2. sigmoid函数的实现</a>
<ul>
<li><a href="#sigmoid%E5%87%BD%E6%95%B0%E5%92%8C%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0%E7%9A%84%E6%AF%94%E8%BE%83">sigmoid函数和阶跃函数的比较</a></li>
</ul>
</li>
<li><a href="#part-3-relu%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9E%E7%8E%B0">Part 3. ReLU函数的实现</a></li>
<li><a href="#part-4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%85%E7%A7%AF">Part 4. 神经网络的内积</a></li>
<li><a href="#part-5-3%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0">Part 5. 3层神经网络的实现</a>
<ul>
<li><a href="#1-%E4%BB%8E%E5%AE%9A%E4%B9%89%E7%AC%A6%E5%8F%B7%E5%BC%80%E5%A7%8B">1. 从定义符号开始：</a></li>
<li><a href="#2-%E5%90%84%E5%B1%82%E9%97%B4%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92%E7%9A%84%E5%AE%9E%E7%8E%B0">2. 各层间信号传递的实现</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88%E5%B0%8F%E7%BB%93">代码整合小结</a></li>
</ul>
</li>
<li><a href="#part-6-%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E8%AE%BE%E8%AE%A1">Part 6. 输出层的设计</a>
<ul>
<li><a href="#softmax%E5%87%BD%E6%95%B0">softmax函数</a></li>
<li><a href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">注意事项</a></li>
<li><a href="#softmax%E5%87%BD%E6%95%B0%E7%9A%84%E7%89%B9%E5%BE%81">softmax函数的特征</a></li>
</ul>
</li>
<li><a href="#part-7-%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E7%A8%8B%E5%BA%8F">Part 7. 数字识别程序</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://CTerena.github.io/post/gan-zhi-ji-perceptronxue-xi-bi-ji/">
              <h3 class="post-title">
                感知机(perceptron)学习笔记
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  与君同是江南客。梦中游，觉来清赏，同作飞梭掷。
  <a class="rss" href="https://CTerena.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
